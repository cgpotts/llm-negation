{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44fd08a-14c1-470d-a251-794f65856110",
   "metadata": {},
   "source": [
    "# What does GPT-3 really understand about negation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff6bd20-a37e-47bd-be68-4485fdf11d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "\n",
    "__date__ = \"2023-01-10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b57ac54-9eb2-44f1-88a8-a5cc5495c342",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7eb1f00-ced7-4a51-bac8-609da8210ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28637e19-10a6-4ba2-88d1-e1d163b110da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add you API key here to re-run the experiments:\n",
    "\n",
    "openai.api_key = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd7b2ef-0b03-48c0-8674-169fecc3e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 999)\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_columns', 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42d842-e413-4de6-9739-0737e7325427",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a02f62d-5faa-49c3-89f4-e257c87c6375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dirname=\"temp/monli/scone\"):\n",
    "    filenames = glob.glob(f\"{dirname}/nmonli_test_edited_full_*.csv\")\n",
    "    dfs = []\n",
    "    for filename in filenames:\n",
    "        split = os.path.basename(filename).replace(\"nmonli_test_edited_full_\", \"\").replace(\".csv\", \"\")\n",
    "        df = pd.read_csv(filename, index_col=0)\n",
    "        df['split'] = split\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs)\n",
    "    \n",
    "DATASET = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a40bcab4-b017-43b1-92ea-dcf511a9ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1_colname = 'sentence1_edited'\n",
    "sentence2_colname = 'sentence2_edited'\n",
    "gold_colname = 'gold_label_edited'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b9807c1-0270-44fc-b008-d7808d5c48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = DATASET[['split', sentence1_colname, sentence2_colname, gold_colname]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e93e0c0a-d886-4056-b509-ab6f403be27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = DATASET.groupby('split').apply(lambda s: s.groupby(gold_colname).sample(1, random_state=1)).reset_index(drop=True)\n",
    "overview = overview[['split', sentence1_colname, gold_colname, sentence2_colname]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "358c83cf-8f75-4ff1-ab07-bf52f2e94cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "             split &                                                                              sentence1\\_edited & gold\\_label\\_edited &                                                                                   sentence2\\_edited \\\\\n",
      "\\midrule\n",
      "    both\\_not\\_scope &                           A dog not weaing a collar caught a basketball moving not very fast. &        entailment &                                      A dog not weaing a collar caught a ball moving not very fast. \\\\\n",
      "    both\\_not\\_scope &                      The girl, not the boy, got a stuffed dog as a gift that was not wrapped. &           neutral &                      The girl, not the boy, got a stuffed malamute as a gift that was not wrapped. \\\\\n",
      "        double\\_neg &                                            It is a lie that dog did not catch any basketball. &        entailment &                                                       It is a lie that dog did not catch any ball. \\\\\n",
      "        double\\_neg &                        It is not the case that the girl will not get a stuffed dog as a gift. &           neutral &                        It is not the case that the girl will not get a stuffed malamute as a gift. \\\\\n",
      "        not\\_scoped &                                                 A dog caught the basketball but not the bird. &        entailment &                                                            A dog caught the ball but not the bird. \\\\\n",
      "        not\\_scoped &  The girl is getting a stuffed dog as a gift, but she is not sure if her friend will like it. &           neutral &  The girl is getting a stuffed malamute as a gift, but she is not sure if her friend will like it. \\\\\n",
      " one\\_scope\\_one\\_not &                                           A dog not on the playground did not catch any ball. &        entailment &                                          A dog not on the playground did not catch any basketball. \\\\\n",
      " one\\_scope\\_one\\_not &      The girl will not get a stuffed malamute as a gift, but not because she failed the exam. &           neutral &                The girl will not get a stuffed dog as a gift, but not because she failed the exam. \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LaTeX version of the above for the write-up:\n",
    "\n",
    "print(overview.to_latex(index=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0990776b-b924-47a9-9983-6ef3d0cd4701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entailment    400\n",
       "neutral       400\n",
       "Name: gold_label_edited, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET[gold_colname].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e25ef09c-5158-4a02-8e1e-c2dd230e68e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "{} &  split \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &    200 \\\\\n",
      "one\\_scope\\_one\\_not &    200 \\\\\n",
      "double\\_neg        &    200 \\\\\n",
      "not\\_scoped        &    200 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(DATASET.split.value_counts().to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2baf5b3b-da8c-49ec-bc27-0e5e4ca78152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "gold\\_label\\_edited &  entailment &  neutral \\\\\n",
      "split             &             &          \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         100 &      100 \\\\\n",
      "double\\_neg        &         100 &      100 \\\\\n",
      "not\\_scoped        &         100 &      100 \\\\\n",
      "one\\_scope\\_one\\_not &         100 &      100 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.crosstab(DATASET.split, DATASET[gold_colname]).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea844d-60b3-45c4-984a-802128f2fbe3",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da9881e9-5149-43ce-8590-c5bdb88f3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_example = {sentence1_colname: \"We didn't eat pizza\", sentence2_colname: \"We didn't eat food\", 'split': 'one_scope_one_not'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2405f1-be8b-457f-a4d5-6ce51aae75f0",
   "metadata": {},
   "source": [
    "### Conditional questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca3745c-d170-4742-a78a-f392343509ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_question_prompt(row, i=None):   \n",
    "    prem = _mid_sentence_normalization(row[sentence1_colname])\n",
    "    hyp = _mid_sentence_normalization(row[sentence2_colname])    \n",
    "    if i is None:\n",
    "        prompt = f\"Is it true that if {prem}, then {hyp}?\"\n",
    "    # Few-shot demonstration case:\n",
    "    else:\n",
    "        ans = \"Yes\" if row[gold_colname] == 'entailment' else \"Maybe\"\n",
    "        prompt = f\"Q{i}: Is it true that if {prem}, then {hyp}?\\nA{i}: {ans}\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def _mid_sentence_normalization(s):\n",
    "    s = s[0].lower() + s[1: ]\n",
    "    s = s.rstrip(\".\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebbf9142-5b38-4280-b64b-77d848032e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it true that if we didn't eat pizza, then we didn't eat food?\n"
     ]
    }
   ],
   "source": [
    "print(conditional_question_prompt(toy_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7660961-3c68-4107-84ca-0df3bad375f3",
   "metadata": {},
   "source": [
    "### Few-shot conditional questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffc0d6d8-b935-41a6-8857-4cb32c9297ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_conditional_question_prompt(row, n=3):\n",
    "    splitname = row['split']\n",
    "    demos = _sample_demos(row, n)\n",
    "    strs = [conditional_question_prompt(row, i+1) for i, row in demos.iterrows()]\n",
    "    strs.append(\"Q: \" + conditional_question_prompt(row) + \"\\nA:\")\n",
    "    return \"\\n\\n\".join(strs)\n",
    "\n",
    "\n",
    "def _sample_demos(row, n):\n",
    "    # Demonstrations are different examples from the same split,\n",
    "    # and we get 2 per label:\n",
    "    split = DATASET[\n",
    "        (DATASET.split == row['split']) & \n",
    "        (DATASET[sentence1_colname] != row[sentence1_colname]) & \n",
    "        (DATASET[sentence2_colname] != row[sentence2_colname])\n",
    "    ]        \n",
    "    demos = split.groupby(gold_colname).sample(2).reset_index()\n",
    "    return demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e68fb8a0-d1a9-4f71-8a25-662ceec6fe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Is it true that if a not so tall person reading a paper is not currently sitting inside a building, then a not so tall person reading a paper is not currently sitting inside a cinema?\n",
      "A1: Yes\n",
      "\n",
      "Q2: Is it true that if a not so tall person reading a paper is not currently sitting inside a building, then a not so tall person reading a paper is not currently sitting inside a cathedral?\n",
      "A2: Yes\n",
      "\n",
      "Q3: Is it true that if a not so tall person reading a paper is not currently sitting inside a manor, then a not so tall person reading a paper is not currently sitting inside a building?\n",
      "A3: Maybe\n",
      "\n",
      "Q4: Is it true that if the girl will not get a stuffed dog as a gift, but not because she failed the exam, then the girl will not get a stuffed mammal as a gift, but not because she failed the exam?\n",
      "A4: Maybe\n",
      "\n",
      "Q: Is it true that if we didn't eat pizza, then we didn't eat food?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_conditional_question_prompt(toy_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404b401-6c02-4c31-8a99-3482b8a34503",
   "metadata": {},
   "source": [
    "### Hypothesis questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3faf5c1b-9e49-4606-bff7-90d8a0b868d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis_question_prompt(row, i=None):   \n",
    "    prem = _mid_sentence_normalization(row[sentence1_colname])\n",
    "    hyp = _mid_sentence_normalization(row[sentence2_colname])    \n",
    "    if i is None:\n",
    "        prompt = f\"Assume that {prem}. Is it then definitely true that {hyp}? Answer Yes or No.\"\n",
    "    # Few-shot demonstration case:\n",
    "    else:\n",
    "        ans = \"Yes\" if row[gold_colname] == 'entailment' else \"No\"\n",
    "        prompt = f\"Q{i}: Assume that {prem}. Is it then definitely true that {hyp}? Answer Yes or No.\\nA{i}: {ans}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8f4819b-198f-457e-b50c-dfdd802903d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume that we didn't eat pizza. Is it then definitely true that we didn't eat food? Answer Yes or No.\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis_question_prompt(toy_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646d6c3-9663-4036-a3d3-31c8a30fa649",
   "metadata": {},
   "source": [
    "### Few-shot hypothesis questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6373c5f3-89b1-4a72-8f77-0278070d4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_hypothesis_question_prompt(row, n=3):\n",
    "    splitname = row['split']\n",
    "    demos = _sample_demos(row, n)\n",
    "    strs = [hypothesis_question_prompt(row, i+1) for i, row in demos.iterrows()]\n",
    "    strs.append(\"Q: \" + hypothesis_question_prompt(row) + \"\\nA:\")\n",
    "    return \"\\n\\n\".join(strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c451d0cc-ad56-44fe-b978-3f2b5f810f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Assume that the players who did not score did not have a ball. Is it then definitely true that the players who did not score did not have a basketball? Answer Yes or No.\n",
      "A1: Yes\n",
      "\n",
      "Q2: Assume that the man does not own a mammal and does not own a cat. Is it then definitely true that the man does not own a dog and does not own a cat? Answer Yes or No.\n",
      "A2: Yes\n",
      "\n",
      "Q3: Assume that a dog not on the playground did not catch any racquetball. Is it then definitely true that a dog not on the playground did not catch any ball? Answer Yes or No.\n",
      "A3: No\n",
      "\n",
      "Q4: Assume that the man does not own a rottweiler and does not own a cat. Is it then definitely true that the man does not own a dog and does not own a cat? Answer Yes or No.\n",
      "A4: No\n",
      "\n",
      "Q: Assume that we didn't eat pizza. Is it then definitely true that we didn't eat food? Answer Yes or No.\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_hypothesis_question_prompt(toy_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31bfb1-5ddd-4c35-9084-f32afd570166",
   "metadata": {},
   "source": [
    "### Conditional truth evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8bfe071-30b3-4ae2-a233-3a8cc5924132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_truth_evaluation_prompt(row, i=None):\n",
    "    prem = _mid_sentence_normalization(row[sentence1_colname])\n",
    "    hyp = _mid_sentence_normalization(row[sentence2_colname])\n",
    "    if i is None:    \n",
    "        prompt = f\"If {prem}, then {hyp}. Is this true?\" \n",
    "    # Few-shot demonstration case:\n",
    "    else:\n",
    "        ans = \"Yes\" if row[gold_colname] == 'entailment' else \"Maybe\"\n",
    "        prompt = f\"C{i}: If {prem}, then {hyp}. Is this true?\\nA{i}: {ans}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76436daa-f2a4-440e-a481-f6afcbe0a8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we didn't eat pizza, then we didn't eat food. Is this true?\n"
     ]
    }
   ],
   "source": [
    "print(conditional_truth_evaluation_prompt(toy_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1385bbf-1543-4ed9-8931-9d2772ed99dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_conditional_truth_evaluation_prompt(row, n=3):\n",
    "    splitname = row['split']\n",
    "    demos = _sample_demos(row, n)\n",
    "    strs = [conditional_truth_evaluation_prompt(row, i+1) for i, row in demos.iterrows()]\n",
    "    strs.append(\"C:\" + conditional_truth_evaluation_prompt(row) + \"\\nA:\")\n",
    "    return \"\\n\\n\".join(strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87853674-4859-4e31-a1a4-ddeee1783536",
   "metadata": {},
   "source": [
    "### Few-shot conditional truth evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b253da1d-f86f-4a13-8312-1bf4e16a851a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1: If the man does not own a dog and does not own a cat, then the man does not own a maltese and does not own a cat. Is this true?\n",
      "A1: Yes\n",
      "\n",
      "C2: If the man does not own a dog and does not own a cat, then the man does not own a terrier and does not own a cat. Is this true?\n",
      "A2: Yes\n",
      "\n",
      "C3: If a not so tall person reading a paper is not currently sitting inside a hotel, then a not so tall person reading a paper is not currently sitting inside a building. Is this true?\n",
      "A3: Maybe\n",
      "\n",
      "C4: If a not so tall person reading a paper is not currently sitting inside a steakhouse, then a not so tall person reading a paper is not currently sitting inside a building. Is this true?\n",
      "A4: Maybe\n",
      "\n",
      "C:If we didn't eat pizza, then we didn't eat food. Is this true?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_conditional_truth_evaluation_prompt(toy_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a95b0f-1800-4995-82c0-0e5f8b04fbc4",
   "metadata": {},
   "source": [
    "### Brown et al.-style\n",
    "\n",
    "Adapted from https://arxiv.org/abs/2005.14165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3139744-5354-4ee6-9d9f-b63afcad3850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brown_et_al_style_prompt(row, i=None):\n",
    "    prem = row[sentence1_colname]\n",
    "    hyp = row[sentence2_colname]\n",
    "    hyp = hyp.rstrip(\".\")\n",
    "    if i is None:\n",
    "        prompt = f\"C: {prem}\\nQ: {hyp}. Yes, No, or Maybe?\"\n",
    "    # Few-shot demonstration case:\n",
    "    else:\n",
    "        ans = \"Yes\" if row[gold_colname] == 'entailment' else \"Maybe\"\n",
    "        prompt = f\"C{i}: {prem}\\nQ{i}: {hyp}. Yes, No, or Maybe?\\nA{i+1}: {ans}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "988a0cf7-5315-420b-a19f-f70b0d26aa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: We didn't eat pizza\n",
      "Q: We didn't eat food. Yes, No, or Maybe?\n"
     ]
    }
   ],
   "source": [
    "print(brown_et_al_style_prompt(toy_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e28133-b706-470a-8e39-25a24b2b2fa9",
   "metadata": {},
   "source": [
    "### Few-shot Brown et al.-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab681a7f-cbe5-4754-8f1a-5d175b1cdeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_brown_et_al_style_prompt(row, n=3):\n",
    "    demos = _sample_demos(row, n) \n",
    "    strs = [brown_et_al_style_prompt(row, i+1) for i, row in demos.iterrows()]\n",
    "    strs.append(brown_et_al_style_prompt(row) + \"\\nA:\")\n",
    "    return \"\\n\\n\".join(strs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c4ebd59-30df-42aa-95fa-7389371e47ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1: The girl will not get a stuffed dog as a gift, but not because she failed the exam.\n",
      "Q1: The girl will not get a stuffed hound as a gift, but not because she failed the exam. Yes, No, or Maybe?\n",
      "A2: Yes\n",
      "\n",
      "C2: The girl will not get a stuffed dog as a gift, but not because she failed the exam.\n",
      "Q2: The girl will not get a stuffed chihuahua as a gift, but not because she failed the exam. Yes, No, or Maybe?\n",
      "A3: Yes\n",
      "\n",
      "C3: A dog not on the playground did not catch any baseball.\n",
      "Q3: A dog not on the playground did not catch any ball. Yes, No, or Maybe?\n",
      "A4: Maybe\n",
      "\n",
      "C4: The girl will not get a stuffed beagle as a gift, but not because she failed the exam.\n",
      "Q4: The girl will not get a stuffed dog as a gift, but not because she failed the exam. Yes, No, or Maybe?\n",
      "A5: Maybe\n",
      "\n",
      "C: We didn't eat pizza\n",
      "Q: We didn't eat food. Yes, No, or Maybe?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_brown_et_al_style_prompt(toy_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e812dd-f329-40df-8be8-8a04a06848b3",
   "metadata": {},
   "source": [
    "### Structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6633569d-f3b6-449a-81b6-688051b36775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_prompt(row, i=None):\n",
    "    prem = row[sentence1_colname]\n",
    "    hyp = row[sentence2_colname]    \n",
    "    if i is None:    \n",
    "        prompt = f\"P: {prem}\\nH: {hyp}\\nL:\"\n",
    "    # Few-shot demonstration case:\n",
    "    else:\n",
    "        prompt = f\"P{i}: {prem}\\nH{i}: {hyp}\\nL{i}: {row[gold_colname]}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2643a0b-1475-4e3c-a288-a59bdf5a4cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: We didn't eat pizza\n",
      "H: We didn't eat food\n",
      "L:\n"
     ]
    }
   ],
   "source": [
    "print(structured_prompt(toy_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d6307-c100-42f1-a82b-63d63558c379",
   "metadata": {},
   "source": [
    "### Few-shot structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edbcc465-b51d-4cbd-b854-dd8fde836dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_structured_prompt(row, n=3):\n",
    "    demos = _sample_demos(row, n)    \n",
    "    strs = [structured_prompt(row, i+1) for i, row in demos.iterrows()]\n",
    "    strs.append(structured_prompt(row))\n",
    "    return \"\\n\\n\".join(strs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7f1bfa5-2d08-45f4-85f4-4e8124e4ad82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1: the man does not own a dog and does not own a cat.\n",
      "H1: the man does not own a malamute and does not own a cat.\n",
      "L1: entailment\n",
      "\n",
      "P2: The girl will not get a stuffed dog as a gift, but not because she failed the exam.\n",
      "H2: The girl will not get a stuffed dachshund as a gift, but not because she failed the exam.\n",
      "L2: entailment\n",
      "\n",
      "P3: A not so tall person reading a paper is not currently sitting inside a hospital.\n",
      "H3: A not so tall person reading a paper is not currently sitting inside a building.\n",
      "L3: neutral\n",
      "\n",
      "P4: A dog not on the playground did not catch any baseball.\n",
      "H4: A dog not on the playground did not catch any ball.\n",
      "L4: neutral\n",
      "\n",
      "P: We didn't eat pizza\n",
      "H: We didn't eat food\n",
      "L:\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_structured_prompt(toy_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131f3e8-afbb-46c7-93c2-9bb82ff29a61",
   "metadata": {},
   "source": [
    "### Reasoning\n",
    "\n",
    "Loosely inspired by https://arxiv.org/pdf/2102.07350.pdf, https://arxiv.org/pdf/2201.11903.pdf, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86a1ab61-c674-44a8-9bb7-bbdde088e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning_prompt(row):\n",
    "    prem = row[sentence1_colname]\n",
    "    hyp = row[sentence2_colname]\n",
    "    prem = _mid_sentence_normalization(prem)\n",
    "    hyp = _mid_sentence_normalization(hyp)\n",
    "    prompt = f\"\"\"Logical and commonsense reasoning exam.\n",
    "\n",
    "Explain your reasoning in detail, then answer with Yes or No. Your answers should follow this 4-line format:\n",
    "\n",
    "Premise: <a tricky logical statement about the world>.\n",
    "Question: <question requiring logical deduction>.\n",
    "Reasoning: <an explanation of what you understand about the possible scenarios>.\n",
    "Answer: <Yes or No>.\n",
    "\n",
    "Premise: {prem}\n",
    "Question: Can we logically conclude for sure that {hyp}?\n",
    "Reasoning: Let's think logically step by step. The premise basically tells us that\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f107c67-e0a6-4652-b8d0-1750a7343adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical and commonsense reasoning exam.\n",
      "\n",
      "Explain your reasoning in detail, then answer with Yes or No. Your answers should follow this 4-line format:\n",
      "\n",
      "Premise: <a tricky logical statement about the world>.\n",
      "Question: <question requiring logical deduction>.\n",
      "Reasoning: <an explanation of what you understand about the possible scenarios>.\n",
      "Answer: <Yes or No>.\n",
      "\n",
      "Premise: we didn't eat pizza\n",
      "Question: Can we logically conclude for sure that we didn't eat food?\n",
      "Reasoning: Let's think logically step by step. The premise basically tells us that\n"
     ]
    }
   ],
   "source": [
    "print(reasoning_prompt(toy_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54219e7-9434-4246-b753-f7d7f23d5197",
   "metadata": {},
   "source": [
    "## Label inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc677963-dd7b-432a-8466-3e25e9c2cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt3(prompts, engine=\"text-davinci-002\", temperature=0.0, batch_size=10, max_tokens=200):    \n",
    "    all_responses = []    \n",
    "    for i in range(0, len(prompts), batch_size):        \n",
    "        response = completion_with_backoff(\n",
    "            engine=engine,       \n",
    "            prompt=prompts[i: i+batch_size],\n",
    "            temperature=temperature,\n",
    "            echo=False,\n",
    "            max_tokens=max_tokens,\n",
    "            n=1)\n",
    "        # We'll keep just the response texts:\n",
    "        all_responses += [d['text'].strip() for d in response['choices']]\n",
    "    return all_responses\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c196e2c2-401f-4510-90a1-ffcd84ac8b00",
   "metadata": {},
   "source": [
    "## Inferring labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e79a18fe-e3c4-4bec-8dc6-d2dc297a2e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_label(s):\n",
    "    yes = re.compile(r\"\\byes\\b\", re.I | re.M)     \n",
    "    if yes.search(s):\n",
    "        return \"entailment\"\n",
    "    else:\n",
    "        return \"neutral\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3165c2-741b-414d-9b20-af86134852c6",
   "metadata": {},
   "source": [
    "## Experiment wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a7585c7-4e72-495d-b74d-3a9b89aecdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(samp, prompt_func, engine, max_tokens=200):\n",
    "    prompt_func_name = prompt_func.__name__        \n",
    "    samp[prompt_func_name] = samp.apply(prompt_func, axis=1)    \n",
    "    responses = run_gpt3(list(samp[prompt_func_name].values), engine=engine, max_tokens=max_tokens)\n",
    "    response_key = engine + \"_\" + prompt_func_name + \"_response\"\n",
    "    pred_key = engine + \"_\" + prompt_func_name + \"_prediction\"    \n",
    "    samp[response_key] = responses    \n",
    "    samp[pred_key] = samp[response_key].apply(infer_label)    \n",
    "    return samp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a5eeb-be53-4ada-9ff4-1f9beadc3cdd",
   "metadata": {},
   "source": [
    "## Results reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e33068bb-c7c8-4593-977d-515a5cb599b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(samp, prompt_func, engine):\n",
    "    accurate = samp[engine + \"_\" + prompt_func.__name__ + \"_prediction\"] == samp[gold_colname]\n",
    "    results = pd.crosstab(samp.split, accurate)\n",
    "    results.loc['All'] = results.sum(axis=0)    \n",
    "    acc = results.apply(lambda row: row[True] / row.sum(), axis=1)        \n",
    "    results['Accuracy'] = acc\n",
    "    # Some clean-up for the LaTeX output:\n",
    "    results = results.rename(columns={True: \"Correct\", False: \"Incorrect\"})\n",
    "    results.columns.name = None\n",
    "    results.index.name = None\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b5fd5-5d1f-4eb3-915b-4cd0ada96874",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd3902-49ad-4811-a676-7126672e4bdc",
   "metadata": {},
   "source": [
    "### Prompt conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d92abfab-03b8-4d4f-b31b-281a5393037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_funcs = (\n",
    "    conditional_question_prompt,\n",
    "    few_shot_conditional_question_prompt,\n",
    "    hypothesis_question_prompt,\n",
    "    few_shot_hypothesis_question_prompt,\n",
    "    conditional_truth_evaluation_prompt,\n",
    "    few_shot_conditional_truth_evaluation_prompt,\n",
    "    brown_et_al_style_prompt,\n",
    "    few_shot_brown_et_al_style_prompt,\n",
    "    structured_prompt,\n",
    "    few_shot_structured_prompt,\n",
    "    reasoning_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee821fb-9dc6-4ae8-8b67-00cfd01dcf4f",
   "metadata": {},
   "source": [
    "### Experiment loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d1d4150-8258-4415-a2ef-0e9466408e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "engines = (\"text-davinci-002\", \"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2388cf04-864b-4d86-9174-da10d7085872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-davinci-002 conditional_question_prompt\n",
      "text-davinci-002 few_shot_conditional_question_prompt\n",
      "text-davinci-002 hypothesis_question_prompt\n",
      "text-davinci-002 few_shot_hypothesis_question_prompt\n",
      "text-davinci-002 conditional_truth_evaluation_prompt\n",
      "text-davinci-002 few_shot_conditional_truth_evaluation_prompt\n",
      "text-davinci-002 brown_et_al_style_prompt\n",
      "text-davinci-002 few_shot_brown_et_al_style_prompt\n",
      "text-davinci-002 structured_prompt\n",
      "text-davinci-002 few_shot_structured_prompt\n",
      "text-davinci-002 reasoning_prompt\n",
      "text-davinci-003 conditional_question_prompt\n",
      "text-davinci-003 few_shot_conditional_question_prompt\n",
      "text-davinci-003 hypothesis_question_prompt\n",
      "text-davinci-003 few_shot_hypothesis_question_prompt\n",
      "text-davinci-003 conditional_truth_evaluation_prompt\n",
      "text-davinci-003 few_shot_conditional_truth_evaluation_prompt\n",
      "text-davinci-003 brown_et_al_style_prompt\n",
      "text-davinci-003 few_shot_brown_et_al_style_prompt\n",
      "text-davinci-003 structured_prompt\n",
      "text-davinci-003 few_shot_structured_prompt\n",
      "text-davinci-003 reasoning_prompt\n"
     ]
    }
   ],
   "source": [
    "for engine in engines:    \n",
    "    for prompt_func in prompt_funcs:\n",
    "        print(engine, prompt_func.__name__)\n",
    "        DATASET = run_experiment(DATASET, prompt_func, max_tokens=200, engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca22120-5e4d-4d96-8a7b-9100616bc73f",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "These are formatted for dirct use in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1348cb63-28b9-4807-be2f-a103076f39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-002: Conditional Question Prompt}\n",
      "\n",
      "\\promptExample{Is it true that if we didn't eat pizza, then we didn't eat food?}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         70 &      130 &      0.65 \\\\\n",
      "double\\_neg        &         99 &      101 &      0.51 \\\\\n",
      "not\\_scoped        &         91 &      109 &      0.55 \\\\\n",
      "one\\_scope\\_one\\_not &        100 &      100 &      0.50 \\\\\n",
      "All               &        360 &      440 &      0.55 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-003: Conditional Question Prompt}\n",
      "\n",
      "\\promptExample{Is it true that if we didn't eat pizza, then we didn't eat food?}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         37 &      163 &      0.81 \\\\\n",
      "double\\_neg        &        100 &      100 &      0.50 \\\\\n",
      "not\\_scoped        &         30 &      170 &      0.85 \\\\\n",
      "one\\_scope\\_one\\_not &        112 &       88 &      0.44 \\\\\n",
      "All               &        279 &      521 &      0.65 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-002: Few-Shot Conditional Question Prompt}\n",
      "\n",
      "\\promptExample{Q1: Is it true that if a dog not on the playground did not catch any ball, then a dog not on the playground did not catch any basketball?\\mynewline\n",
      "A1: Yes\\mynewline\n",
      "\\mynewline\n",
      "Q2: Is it true that if a dog not on the playground did not catch any ball, then a dog not on the playground did not catch any baseball?\\mynewline\n",
      "A2: Yes\\mynewline\n",
      "\\mynewline\n",
      "Q3: Is it true that if a not so tall person reading a paper is not currently sitting inside a house, then a not so tall person reading a paper is not currently sitting inside a building?\\mynewline\n",
      "A3: Maybe\\mynewline\n",
      "\n",
      "Q4: Is it true that if the man does not own a pomeranian and does not own a cat, then the man does not own a dog and does not own a cat?\n",
      "A4: Maybe\n",
      "\n",
      "Q: Is it true that if we didn't eat pizza, then we didn't eat food?\n",
      "A:}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         43 &      157 &      0.79 \\\\\n",
      "double\\_neg        &         97 &      103 &      0.52 \\\\\n",
      "not\\_scoped        &         37 &      163 &      0.81 \\\\\n",
      "one\\_scope\\_one\\_not &        135 &       65 &      0.33 \\\\\n",
      "All               &        312 &      488 &      0.61 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-003: Few-Shot Conditional Question Prompt}\n",
      "\n",
      "\\promptExample{Q1: Is it true that if the players who did not score did not have a ball, then the players who did not score did not have a baseball?\\mynewline\n",
      "A1: Yes\\mynewline\n",
      "\\mynewline\n",
      "Q2: Is it true that if the man does not own a dog and does not own a cat, then the man does not own a pinscher and does not own a cat?\\mynewline\n",
      "A2: Yes\\mynewline\n",
      "\\mynewline\n",
      "Q3: Is it true that if a not so tall person reading a paper is not currently sitting inside a library, then a not so tall person reading a paper is not currently sitting inside a building?\\mynewline\n",
      "A3: Maybe\\mynewline\n",
      "\n",
      "Q4: Is it true that if the girl will not get a stuffed greyhound as a gift, but not because she failed the exam, then the girl will not get a stuffed dog as a gift, but not because she failed the exam?\n",
      "A4: Maybe\n",
      "\n",
      "Q: Is it true that if we didn't eat pizza, then we didn't eat food?\n",
      "A:}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         22 &      178 &      0.89 \\\\\n",
      "double\\_neg        &         84 &      116 &      0.58 \\\\\n",
      "not\\_scoped        &         36 &      164 &      0.82 \\\\\n",
      "one\\_scope\\_one\\_not &        138 &       62 &      0.31 \\\\\n",
      "All               &        280 &      520 &      0.65 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-002: Hypothesis Question Prompt}\n",
      "\n",
      "\\promptExample{Assume that we didn't eat pizza. Is it then definitely true that we didn't eat food? Answer Yes or No.}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         28 &      172 &      0.86 \\\\\n",
      "double\\_neg        &         75 &      125 &      0.62 \\\\\n",
      "not\\_scoped        &         34 &      166 &      0.83 \\\\\n",
      "one\\_scope\\_one\\_not &        120 &       80 &      0.40 \\\\\n",
      "All               &        257 &      543 &      0.68 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-003: Hypothesis Question Prompt}\n",
      "\n",
      "\\promptExample{Assume that we didn't eat pizza. Is it then definitely true that we didn't eat food? Answer Yes or No.}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         59 &      141 &      0.70 \\\\\n",
      "double\\_neg        &         98 &      102 &      0.51 \\\\\n",
      "not\\_scoped        &         20 &      180 &      0.90 \\\\\n",
      "one\\_scope\\_one\\_not &        116 &       84 &      0.42 \\\\\n",
      "All               &        293 &      507 &      0.63 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-002: Few-Shot Hypothesis Question Prompt}\n",
      "\n",
      "\\promptExample{Q1: Assume that the girl will not get a stuffed dog as a gift, but not because she failed the exam. Is it then definitely true that the girl will not get a stuffed husky as a gift, but not because she failed the exam? Answer Yes or No.\\mynewline\n",
      "A1: Yes\\mynewline\n",
      "\\mynewline\n",
      "Q2: Assume that a not so tall person reading a paper is not currently sitting inside a building. Is it then definitely true that a not so tall person reading a paper is not currently sitting inside a steakhouse? Answer Yes or No.\\mynewline\n",
      "A2: Yes\\mynewline\n",
      "\\mynewline\n",
      "Q3: Assume that a not so tall person reading a paper is not currently sitting inside a embassy. Is it then definitely true that a not so tall person reading a paper is not currently sitting inside a building? Answer Yes or No.\\mynewline\n",
      "A3: No\\mynewline\n",
      "\n",
      "Q4: Assume that the girl will not get a stuffed pug as a gift, but not because she failed the exam. Is it then definitely true that the girl will not get a stuffed dog as a gift, but not because she failed the exam? Answer Yes or No.\n",
      "A4: No\n",
      "\n",
      "Q: Assume that we didn't eat pizza. Is it then definitely true that we didn't eat food? Answer Yes or No.\n",
      "A:}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         15 &      185 &      0.93 \\\\\n",
      "double\\_neg        &         63 &      137 &      0.69 \\\\\n",
      "not\\_scoped        &         18 &      182 &      0.91 \\\\\n",
      "one\\_scope\\_one\\_not &        120 &       80 &      0.40 \\\\\n",
      "All               &        216 &      584 &      0.73 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-003: Few-Shot Hypothesis Question Prompt}\n",
      "\n",
      "\\promptExample{Q1: Assume that a not so tall person reading a paper is not currently sitting inside a building. Is it then definitely true that a not so tall person reading a paper is not currently sitting inside a cafe? Answer Yes or No.\\mynewline\n",
      "A1: Yes\\mynewline\n",
      "\\mynewline\n",
      "Q2: Assume that the man, who's eyes are not open, is not steering a car. Is it then definitely true that the man, who's eyes are not open, is not steering a sedan? Answer Yes or No.\\mynewline\n",
      "A2: Yes\\mynewline\n",
      "\\mynewline\n",
      "Q3: Assume that the man does not own a pomeranian and does not own a cat. Is it then definitely true that the man does not own a dog and does not own a cat? Answer Yes or No.\\mynewline\n",
      "A3: No\\mynewline\n",
      "\n",
      "Q4: Assume that a not so tall person reading a paper is not currently sitting inside a court. Is it then definitely true that a not so tall person reading a paper is not currently sitting inside a building? Answer Yes or No.\n",
      "A4: No\n",
      "\n",
      "Q: Assume that we didn't eat pizza. Is it then definitely true that we didn't eat food? Answer Yes or No.\n",
      "A:}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &          9 &      191 &      0.95 \\\\\n",
      "double\\_neg        &         35 &      165 &      0.82 \\\\\n",
      "not\\_scoped        &          6 &      194 &      0.97 \\\\\n",
      "one\\_scope\\_one\\_not &        117 &       83 &      0.41 \\\\\n",
      "All               &        167 &      633 &      0.79 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-002: Conditional Truth Evaluation Prompt}\n",
      "\n",
      "\\promptExample{If we didn't eat pizza, then we didn't eat food. Is this true?}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         69 &      131 &      0.66 \\\\\n",
      "double\\_neg        &         80 &      120 &      0.60 \\\\\n",
      "not\\_scoped        &         72 &      128 &      0.64 \\\\\n",
      "one\\_scope\\_one\\_not &         86 &      114 &      0.57 \\\\\n",
      "All               &        307 &      493 &      0.62 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-003: Conditional Truth Evaluation Prompt}\n",
      "\n",
      "\\promptExample{If we didn't eat pizza, then we didn't eat food. Is this true?}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         39 &      161 &      0.81 \\\\\n",
      "double\\_neg        &        113 &       87 &      0.43 \\\\\n",
      "not\\_scoped        &         28 &      172 &      0.86 \\\\\n",
      "one\\_scope\\_one\\_not &        104 &       96 &      0.48 \\\\\n",
      "All               &        284 &      516 &      0.65 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-002: Few-Shot Conditional Truth Evaluation Prompt}\n",
      "\n",
      "\\promptExample{C1: If the girl will not get a stuffed dog as a gift, but not because she failed the exam, then the girl will not get a stuffed shetland as a gift, but not because she failed the exam. Is this true?\\mynewline\n",
      "A1: Yes\\mynewline\n",
      "\\mynewline\n",
      "C2: If a not so tall person reading a paper is not currently sitting inside a building, then a not so tall person reading a paper is not currently sitting inside a restaurant. Is this true?\\mynewline\n",
      "A2: Yes\\mynewline\n",
      "\\mynewline\n",
      "C3: If the man, who's eyes are not open, is not steering a taxi, then the man, who's eyes are not open, is not steering a car. Is this true?\\mynewline\n",
      "A3: Maybe\\mynewline\n",
      "\n",
      "C4: If a not so tall person reading a paper is not currently sitting inside a monastery, then a not so tall person reading a paper is not currently sitting inside a building. Is this true?\n",
      "A4: Maybe\n",
      "\n",
      "C:If we didn't eat pizza, then we didn't eat food. Is this true?\n",
      "A:}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         23 &      177 &      0.89 \\\\\n",
      "double\\_neg        &         82 &      118 &      0.59 \\\\\n",
      "not\\_scoped        &         22 &      178 &      0.89 \\\\\n",
      "one\\_scope\\_one\\_not &        126 &       74 &      0.37 \\\\\n",
      "All               &        253 &      547 &      0.68 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-003: Few-Shot Conditional Truth Evaluation Prompt}\n",
      "\n",
      "\\promptExample{C1: If a not so tall person reading a paper is not currently sitting inside a building, then a not so tall person reading a paper is not currently sitting inside a clinic. Is this true?\\mynewline\n",
      "A1: Yes\\mynewline\n",
      "\\mynewline\n",
      "C2: If a not so tall person reading a paper is not currently sitting inside a building, then a not so tall person reading a paper is not currently sitting inside a skyscraper. Is this true?\\mynewline\n",
      "A2: Yes\\mynewline\n",
      "\\mynewline\n",
      "C3: If the man does not own a rottweiler and does not own a cat, then the man does not own a dog and does not own a cat. Is this true?\\mynewline\n",
      "A3: Maybe\\mynewline\n",
      "\n",
      "C4: If the players who did not score did not have a racquetball, then the players who did not score did not have a ball. Is this true?\n",
      "A4: Maybe\n",
      "\n",
      "C:If we didn't eat pizza, then we didn't eat food. Is this true?\n",
      "A:}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         18 &      182 &      0.91 \\\\\n",
      "double\\_neg        &         71 &      129 &      0.65 \\\\\n",
      "not\\_scoped        &         17 &      183 &      0.92 \\\\\n",
      "one\\_scope\\_one\\_not &        131 &       69 &      0.34 \\\\\n",
      "All               &        237 &      563 &      0.70 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-002: Brown Et Al Style Prompt}\n",
      "\n",
      "\\promptExample{C: We didn't eat pizza\\mynewline\n",
      "Q: We didn't eat food. Yes, No, or Maybe?}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         82 &      118 &      0.59 \\\\\n",
      "double\\_neg        &         90 &      110 &      0.55 \\\\\n",
      "not\\_scoped        &         81 &      119 &      0.59 \\\\\n",
      "one\\_scope\\_one\\_not &        103 &       97 &      0.48 \\\\\n",
      "All               &        356 &      444 &      0.56 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-003: Brown Et Al Style Prompt}\n",
      "\n",
      "\\promptExample{C: We didn't eat pizza\\mynewline\n",
      "Q: We didn't eat food. Yes, No, or Maybe?}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         52 &      148 &      0.74 \\\\\n",
      "double\\_neg        &         92 &      108 &      0.54 \\\\\n",
      "not\\_scoped        &         60 &      140 &      0.70 \\\\\n",
      "one\\_scope\\_one\\_not &        112 &       88 &      0.44 \\\\\n",
      "All               &        316 &      484 &      0.60 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-002: Few-Shot Brown Et Al Style Prompt}\n",
      "\n",
      "\\promptExample{C1: The man, who's eyes are not open, is not steering a car.\\mynewline\n",
      "Q1: The man, who's eyes are not open, is not steering a jeep. Yes, No, or Maybe?\\mynewline\n",
      "A2: Yes\\mynewline\n",
      "\\mynewline\n",
      "C2: A not so tall person reading a paper is not currently sitting inside a building.\\mynewline\n",
      "Q2: A not so tall person reading a paper is not currently sitting inside a planetarium. Yes, No, or Maybe?\\mynewline\n",
      "A3: Yes\\mynewline\n",
      "\\mynewline\n",
      "C3: A not so tall person reading a paper is not currently sitting inside a skyscraper.\n",
      "Q3: A not so tall person reading a paper is not currently sitting inside a building. Yes, No, or Maybe?\n",
      "A4: Maybe\n",
      "\n",
      "C4: The girl will not get a stuffed doberman as a gift, but not because she failed the exam.\n",
      "Q4: The girl will not get a stuffed dog as a gift, but not because she failed the exam. Yes, No, or Maybe?\n",
      "A5: Maybe\n",
      "\n",
      "C: We didn't eat pizza\n",
      "Q: We didn't eat food. Yes, No, or Maybe?\n",
      "A:}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         43 &      157 &      0.79 \\\\\n",
      "double\\_neg        &         57 &      143 &      0.71 \\\\\n",
      "not\\_scoped        &         50 &      150 &      0.75 \\\\\n",
      "one\\_scope\\_one\\_not &        142 &       58 &      0.29 \\\\\n",
      "All               &        292 &      508 &      0.64 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-003: Few-Shot Brown Et Al Style Prompt}\n",
      "\n",
      "\\promptExample{C1: The girl will not get a stuffed dog as a gift, but not because she failed the exam.\\mynewline\n",
      "Q1: The girl will not get a stuffed schnauzer as a gift, but not because she failed the exam. Yes, No, or Maybe?\\mynewline\n",
      "A2: Yes\\mynewline\n",
      "\\mynewline\n",
      "C2: A not so tall person reading a paper is not currently sitting inside a building.\\mynewline\n",
      "Q2: A not so tall person reading a paper is not currently sitting inside a manor. Yes, No, or Maybe?\\mynewline\n",
      "A3: Yes\\mynewline\n",
      "\\mynewline\n",
      "C3: A not so tall person reading a paper is not currently sitting inside a hospital.\n",
      "Q3: A not so tall person reading a paper is not currently sitting inside a building. Yes, No, or Maybe?\n",
      "A4: Maybe\n",
      "\n",
      "C4: The players who did not score did not have a handball.\n",
      "Q4: The players who did not score did not have a ball. Yes, No, or Maybe?\n",
      "A5: Maybe\n",
      "\n",
      "C: We didn't eat pizza\n",
      "Q: We didn't eat food. Yes, No, or Maybe?\n",
      "A:}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         46 &      154 &      0.77 \\\\\n",
      "double\\_neg        &         31 &      169 &      0.84 \\\\\n",
      "not\\_scoped        &         82 &      118 &      0.59 \\\\\n",
      "one\\_scope\\_one\\_not &        138 &       62 &      0.31 \\\\\n",
      "All               &        297 &      503 &      0.63 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-002: Structured Prompt}\n",
      "\n",
      "\\promptExample{P: We didn't eat pizza\\mynewline\n",
      "H: We didn't eat food\\mynewline\n",
      "L:}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &        100 &      100 &      0.50 \\\\\n",
      "double\\_neg        &        100 &      100 &      0.50 \\\\\n",
      "not\\_scoped        &        100 &      100 &      0.50 \\\\\n",
      "one\\_scope\\_one\\_not &        100 &      100 &      0.50 \\\\\n",
      "All               &        400 &      400 &      0.50 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-003: Structured Prompt}\n",
      "\n",
      "\\promptExample{P: We didn't eat pizza\\mynewline\n",
      "H: We didn't eat food\\mynewline\n",
      "L:}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &        100 &      100 &      0.50 \\\\\n",
      "double\\_neg        &        100 &      100 &      0.50 \\\\\n",
      "not\\_scoped        &        100 &      100 &      0.50 \\\\\n",
      "one\\_scope\\_one\\_not &        100 &      100 &      0.50 \\\\\n",
      "All               &        400 &      400 &      0.50 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-002: Few-Shot Structured Prompt}\n",
      "\n",
      "\\promptExample{P1: the man does not own a dog and does not own a cat.\\mynewline\n",
      "H1: the man does not own a schnauzer and does not own a cat.\\mynewline\n",
      "L1: entailment\\mynewline\n",
      "\\mynewline\n",
      "P2: A not so tall person reading a paper is not currently sitting inside a building.\\mynewline\n",
      "H2: A not so tall person reading a paper is not currently sitting inside a cafeteria.\\mynewline\n",
      "L2: entailment\\mynewline\n",
      "\\mynewline\n",
      "P3: A not so tall person reading a paper is not currently sitting inside a skyscraper.\n",
      "H3: A not so tall person reading a paper is not currently sitting inside a building.\n",
      "L3: neutral\n",
      "\n",
      "P4: A not so tall person reading a paper is not currently sitting inside a motel.\n",
      "H4: A not so tall person reading a paper is not currently sitting inside a building.\n",
      "L4: neutral\n",
      "\n",
      "P: We didn't eat pizza\n",
      "H: We didn't eat food\n",
      "L:}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &        100 &      100 &      0.50 \\\\\n",
      "double\\_neg        &        100 &      100 &      0.50 \\\\\n",
      "not\\_scoped        &        100 &      100 &      0.50 \\\\\n",
      "one\\_scope\\_one\\_not &        100 &      100 &      0.50 \\\\\n",
      "All               &        400 &      400 &      0.50 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-003: Few-Shot Structured Prompt}\n",
      "\n",
      "\\promptExample{P1: A not so tall person reading a paper is not currently sitting inside a building.\\mynewline\n",
      "H1: A not so tall person reading a paper is not currently sitting inside a villa.\\mynewline\n",
      "L1: entailment\\mynewline\n",
      "\\mynewline\n",
      "P2: A not so tall person reading a paper is not currently sitting inside a building.\\mynewline\n",
      "H2: A not so tall person reading a paper is not currently sitting inside a inn.\\mynewline\n",
      "L2: entailment\\mynewline\n",
      "\\mynewline\n",
      "P3: A dog not on the playground did not catch any handball.\n",
      "H3: A dog not on the playground did not catch any ball.\n",
      "L3: neutral\n",
      "\n",
      "P4: The girl will not get a stuffed poodle as a gift, but not because she failed the exam.\n",
      "H4: The girl will not get a stuffed dog as a gift, but not because she failed the exam.\n",
      "L4: neutral\n",
      "\n",
      "P: We didn't eat pizza\n",
      "H: We didn't eat food\n",
      "L:}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &        100 &      100 &      0.50 \\\\\n",
      "double\\_neg        &        100 &      100 &      0.50 \\\\\n",
      "not\\_scoped        &        100 &      100 &      0.50 \\\\\n",
      "one\\_scope\\_one\\_not &        100 &      100 &      0.50 \\\\\n",
      "All               &        400 &      400 &      0.50 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-002: Reasoning Prompt}\n",
      "\n",
      "\\promptExample{Logical and commonsense reasoning exam.\\mynewline\n",
      "\\mynewline\n",
      "Explain your reasoning in detail, then answer with Yes or No. Your answers should follow this 4-line format:\\mynewline\n",
      "\\mynewline\n",
      "Premise: <a tricky logical statement about the world>.\\mynewline\n",
      "Question: <question requiring logical deduction>.\\mynewline\n",
      "Reasoning: <an explanation of what you understand about the possible scenarios>.\\mynewline\n",
      "Answer: <Yes or No>.\\mynewline\n",
      "\n",
      "Premise: we didn't eat pizza\n",
      "Question: Can we logically conclude for sure that we didn't eat food?\n",
      "Reasoning: Let's think logically step by step. The premise basically tells us that}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         63 &      137 &      0.69 \\\\\n",
      "double\\_neg        &         77 &      123 &      0.61 \\\\\n",
      "not\\_scoped        &         61 &      139 &      0.69 \\\\\n",
      "one\\_scope\\_one\\_not &         87 &      113 &      0.56 \\\\\n",
      "All               &        288 &      512 &      0.64 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\\subsection{text-davinci-003: Reasoning Prompt}\n",
      "\n",
      "\\promptExample{Logical and commonsense reasoning exam.\\mynewline\n",
      "\\mynewline\n",
      "Explain your reasoning in detail, then answer with Yes or No. Your answers should follow this 4-line format:\\mynewline\n",
      "\\mynewline\n",
      "Premise: <a tricky logical statement about the world>.\\mynewline\n",
      "Question: <question requiring logical deduction>.\\mynewline\n",
      "Reasoning: <an explanation of what you understand about the possible scenarios>.\\mynewline\n",
      "Answer: <Yes or No>.\\mynewline\n",
      "\n",
      "Premise: we didn't eat pizza\n",
      "Question: Can we logically conclude for sure that we didn't eat food?\n",
      "Reasoning: Let's think logically step by step. The premise basically tells us that}\n",
      "\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Incorrect &  Correct &  Accuracy \\\\\n",
      "\\midrule\n",
      "both\\_not\\_scope    &         14 &      186 &      0.93 \\\\\n",
      "double\\_neg        &         57 &      143 &      0.71 \\\\\n",
      "not\\_scoped        &         24 &      176 &      0.88 \\\\\n",
      "one\\_scope\\_one\\_not &        107 &       93 &      0.47 \\\\\n",
      "All               &        202 &      598 &      0.75 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt_func in prompt_funcs:\n",
    "    for engine in engines:\n",
    "        print(\"%\" * 70, end=\"\\n\")    \n",
    "        sechead = prompt_func.__name__.replace(\"_\", \" \").title().replace(\"Few Shot\", \"Few-Shot\")\n",
    "        print(f\"\\\\subsection{{{engine}: {sechead}}}\",  end=\"\\n\\n\") \n",
    "        p = prompt_func(toy_example)\n",
    "        p = re.sub(r\"\\n\", r\"\\\\mynewline\\n\", p, re.M)         \n",
    "        print(f\"\\\\promptExample{{{p}}}\", end=\"\\n\\n\")        \n",
    "        print(\"\\\\begin{center}\")\n",
    "        print(report_results(DATASET, prompt_func, engine).to_latex(float_format=\"%.2f\"), end=\"\")\n",
    "        print(\"\\\\end{center}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9af862-6fc7-4ad0-ab83-16963398a7b8",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "188b9664-c636-4485-9921-21bde3071df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(samp, prompt_func, engine, splitname):\n",
    "    prompt_func_name = prompt_func.__name__\n",
    "    pred_colname = engine + \"_\" + prompt_func_name + \"_prediction\"\n",
    "    response_colname = engine + \"_\" + prompt_func_name + \"_response\"\n",
    "    err_df = samp[(samp[pred_colname] != samp[gold_colname]) & (samp.split == splitname)]\n",
    "    return err_df[[sentence1_colname, gold_colname, sentence2_colname, prompt_func_name, pred_colname, response_colname]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc31ec52-da4e-4586-ad32-a0417b51911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_analysis(DATASET, reasoning_prompt, \"text-davinci-002\", \"one_scope_one_not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bb82821-5f83-470b-98d4-8ab3c63cbb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET.to_json(\"scone-davinci-results.json\", orient='records', indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
